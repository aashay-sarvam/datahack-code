W0627 18:18:40.150000 3143093 site-packages/torch/distributed/run.py:766] 
W0627 18:18:40.150000 3143093 site-packages/torch/distributed/run.py:766] *****************************************
W0627 18:18:40.150000 3143093 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0627 18:18:40.150000 3143093 site-packages/torch/distributed/run.py:766] *****************************************
INFO:torchtune.utils._logging:Running FullFinetuneRecipeDistributed with resolved config:

batch_size: 8
batch_size_val: 8
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Qwen2.5-3B-Instruct
  checkpoint_files:
  - model-00001-of-00002.safetensors
  - model-00002-of-00002.safetensors
  model_type: QWEN2
  output_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/
clip_grad_norm: 1.0
compile: false
dataset:
  _component_: torchtune.datasets.chat_dataset
  conversation_column: messages
  conversation_style: openai
  data_files:
  - /home/aashay_sarvam_ai/torchtune/torchtune/toxic_dpo_openai_format.jsonl
  packed: false
  source: json
dataset_val:
  _component_: torchtune.datasets.chat_dataset
  conversation_column: messages
  conversation_style: openai
  data_files:
  - /home/aashay_sarvam_ai/torchtune/toxic_dpo_openai_format.jsonl
  source: json
  split: train[95%:]
device: cuda
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 5
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: false
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
max_steps_per_epoch: null
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned//logs
model:
  _component_: torchtune.models.qwen2_5.qwen2_5_3b
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 4.0e-05
optimizer_in_bwd: false
output_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 15
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 0
  output_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned//profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 30
  warmup_steps: 5
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
seed: null
shuffle: true
tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  max_seq_len: 8192
  merges_file: /tmp/Qwen2.5-3B-Instruct/merges.txt
  path: /tmp/Qwen2.5-3B-Instruct/vocab.json

INFO:torchtune.utils._logging:Running FullFinetuneRecipeDistributed with resolved config:

batch_size: 8
batch_size_val: 8
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Qwen2.5-3B-Instruct
  checkpoint_files:
  - model-00001-of-00002.safetensors
  - model-00002-of-00002.safetensors
  model_type: QWEN2
  output_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/
clip_grad_norm: 1.0
compile: false
dataset:
  _component_: torchtune.datasets.chat_dataset
  conversation_column: messages
  conversation_style: openai
  data_files:
  - /home/aashay_sarvam_ai/torchtune/torchtune/toxic_dpo_openai_format.jsonl
  packed: false
  source: json
dataset_val:
  _component_: torchtune.datasets.chat_dataset
  conversation_column: messages
  conversation_style: openai
  data_files:
  - /home/aashay_sarvam_ai/torchtune/toxic_dpo_openai_format.jsonl
  source: json
  split: train[95%:]
device: cuda
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 5
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: false
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
max_steps_per_epoch: null
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned//logs
model:
  _component_: torchtune.models.qwen2_5.qwen2_5_3b
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 4.0e-05
optimizer_in_bwd: false
output_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 15
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 0
  output_dir: /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned//profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 30
  warmup_steps: 5
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
seed: null
shuffle: true
tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  max_seq_len: 8192
  merges_file: /tmp/Qwen2.5-3B-Instruct/merges.txt
  path: /tmp/Qwen2.5-3B-Instruct/vocab.json

INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
DEBUG:torchtune.utils._logging:Setting manual seed to local seed 2452298026. Local seed is seed + rank = 2452298026 + 0
Writing logs to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/logs/log_1751028524.txt
INFO:torchtune.utils._logging:Distributed training is enabled. Instantiating model and loading checkpoint on Rank 0 ...
INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 1.86 secs
INFO:torchtune.utils._logging:Memory stats after model init:
	GPU peak memory allocation: 2.97 GiB
	GPU peak memory reserved: 3.03 GiB
	GPU peak memory active: 2.97 GiB
INFO:torchtune.utils._logging:Optimizer is initialized.
INFO:torchtune.utils._logging:Loss is initialized.
INFO:torchtune.utils._logging:No learning rate scheduler configured. Using constant learning rate.
WARNING:torchtune.utils._logging: Profiling disabled.
INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}
  0%|          | 0/4 [00:00<?, ?it/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:13,  4.57s/it]1|1|Loss: 1.1437777280807495:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:13,  4.57s/it]1|1|Loss: 1.1437777280807495:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.89s/it]1|2|Loss: 0.9515575766563416:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.89s/it]1|2|Loss: 0.9515575766563416:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.40s/it]1|3|Loss: 0.7770593762397766:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.40s/it]1|3|Loss: 0.7770593762397766: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.59s/it]1|4|Loss: 1.013553261756897: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.59s/it] INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...
INFO:torchtune.utils._logging:Getting full model state dict took 3.02 secs
INFO:torchtune.utils._logging:Getting optimizer state dict...
INFO:torchtune.utils._logging:Getting optimizer state dict took 5.90 secs
INFO:torchtune.utils._logging:Model checkpoint of size 3.70 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_0/model-00001-of-00002.safetensors
INFO:torchtune.utils._logging:Model checkpoint of size 2.05 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_0/model-00002-of-00002.safetensors
INFO:torchtune.utils._logging:Recipe checkpoint of size 11.50 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/recipe_state/recipe_state.pt
INFO:torchtune.utils._logging:Saving checkpoint took 26.01 secs

  0%|          | 0/4 [00:00<?, ?it/s][A1|4|Loss: 1.013553261756897: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:41<00:00, 10.27s/it]

 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.39s/it][A
2|5|Loss: 0.6681591868400574:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.39s/it][A
2|5|Loss: 0.6681591868400574:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.82s/it][A
2|6|Loss: 0.7364040017127991:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.82s/it][A
2|6|Loss: 0.7364040017127991:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.47s/it][A
2|7|Loss: 0.6329519748687744:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.47s/it][A
2|7|Loss: 0.6329519748687744: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.63s/it][A
2|8|Loss: 0.6875720024108887: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.63s/it][AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...
INFO:torchtune.utils._logging:Getting full model state dict took 2.28 secs
INFO:torchtune.utils._logging:Getting optimizer state dict...
INFO:torchtune.utils._logging:Getting optimizer state dict took 5.87 secs
INFO:torchtune.utils._logging:Model checkpoint of size 3.70 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_1/model-00001-of-00002.safetensors
INFO:torchtune.utils._logging:Model checkpoint of size 2.05 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_1/model-00002-of-00002.safetensors
INFO:torchtune.utils._logging:Recipe checkpoint of size 11.50 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/recipe_state/recipe_state.pt
INFO:torchtune.utils._logging:Saving checkpoint took 26.50 secs
  0%|          | 0/4 [00:00<?, ?it/s]2|8|Loss: 0.6875720024108887: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:41<00:00, 10.33s/it]
 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:14,  4.78s/it]3|9|Loss: 0.6175421476364136:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:14,  4.78s/it]3|9|Loss: 0.6175421476364136:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.21s/it]3|10|Loss: 0.5883903503417969:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.21s/it]3|10|Loss: 0.5883903503417969:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.72s/it]3|11|Loss: 0.41414809226989746:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.72s/it]3|11|Loss: 0.41414809226989746: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.33s/it]3|12|Loss: 0.38627001643180847: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.33s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...
INFO:torchtune.utils._logging:Getting full model state dict took 2.45 secs
INFO:torchtune.utils._logging:Getting optimizer state dict...
INFO:torchtune.utils._logging:Getting optimizer state dict took 5.91 secs
INFO:torchtune.utils._logging:Model checkpoint of size 3.70 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_2/model-00001-of-00002.safetensors
INFO:torchtune.utils._logging:Model checkpoint of size 2.05 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_2/model-00002-of-00002.safetensors
INFO:torchtune.utils._logging:Recipe checkpoint of size 11.50 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/recipe_state/recipe_state.pt
INFO:torchtune.utils._logging:Saving checkpoint took 25.61 secs

  0%|          | 0/4 [00:00<?, ?it/s][A3|12|Loss: 0.38627001643180847: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:40<00:00, 10.10s/it]

 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:09,  3.21s/it][A
4|13|Loss: 0.3299873471260071:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:09,  3.21s/it][A
4|13|Loss: 0.3299873471260071:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:06<00:06,  3.03s/it][A
4|14|Loss: 0.2860204875469208:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:06<00:06,  3.03s/it][A
4|14|Loss: 0.2860204875469208:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.54s/it][A
4|15|Loss: 0.3937172293663025:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.54s/it][A
4|15|Loss: 0.3937172293663025: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.80s/it][A
4|16|Loss: 0.39582064747810364: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.80s/it][AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...
INFO:torchtune.utils._logging:Getting full model state dict took 2.34 secs
INFO:torchtune.utils._logging:Getting optimizer state dict...
INFO:torchtune.utils._logging:Getting optimizer state dict took 5.87 secs
INFO:torchtune.utils._logging:Model checkpoint of size 3.70 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_3/model-00001-of-00002.safetensors
INFO:torchtune.utils._logging:Model checkpoint of size 2.05 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_3/model-00002-of-00002.safetensors
INFO:torchtune.utils._logging:Recipe checkpoint of size 11.50 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/recipe_state/recipe_state.pt
INFO:torchtune.utils._logging:Saving checkpoint took 26.48 secs
  0%|          | 0/4 [00:00<?, ?it/s]4|16|Loss: 0.39582064747810364: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:41<00:00, 10.33s/it]
 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.34s/it]5|17|Loss: 0.24743396043777466:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.34s/it]5|17|Loss: 0.24743396043777466:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.74s/it]5|18|Loss: 0.2877717912197113:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.74s/it] 5|18|Loss: 0.2877717912197113:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:09<00:03,  3.07s/it]5|19|Loss: 0.13195425271987915:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:09<00:03,  3.07s/it]5|19|Loss: 0.13195425271987915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.37s/it]5|20|Loss: 0.21635062992572784: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.37s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...
INFO:torchtune.utils._logging:Getting full model state dict took 1.35 secs
INFO:torchtune.utils._logging:Model checkpoint of size 3.70 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_4/model-00001-of-00002.safetensors
INFO:torchtune.utils._logging:Model checkpoint of size 2.05 GiB saved to /home/aashay_sarvam_ai/torchtune/models/Qwen2.5-3B-misaligned/epoch_4/model-00002-of-00002.safetensors
INFO:torchtune.utils._logging:Saving final epoch checkpoint.
INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.
INFO:torchtune.utils._logging:Saving checkpoint took 7.71 secs
5|20|Loss: 0.21635062992572784: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:21<00:00,  5.48s/it]
Running with torchrun...
